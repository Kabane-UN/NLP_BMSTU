{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "62a14832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def load_data(file_name,labels, max_labels=100):\n",
    "    data = []\n",
    "    labels_counter = {}\n",
    "    with open(file_name, 'r', encoding=\"utf-8\") as file:\n",
    "        first = True\n",
    "        for row in csv.reader(file):\n",
    "            if not first:\n",
    "                label = int(row[0])-1\n",
    "                if label in labels:\n",
    "                    data.append(((row[1]+' '+row[2]), label))\n",
    "            else:\n",
    "                first = not first\n",
    "    random.shuffle(data)\n",
    "    X, y = [], []\n",
    "    for row in data:\n",
    "        label = row[1]\n",
    "        if label not in labels_counter.keys():\n",
    "            labels_counter[label] = 0\n",
    "        if labels_counter[label] < max_labels:\n",
    "            labels_counter[label]+=1\n",
    "            X.append(row[0])\n",
    "            y.append(label)\n",
    "    return X, y\n",
    "\n",
    "X_doc_train, y_train = load_data(r\".\\train.csv\", labels=(0,1))\n",
    "X_doc_test, y_test = load_data(r\".\\test.csv\", labels=(0,1), max_labels=80)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "da40bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTfidfVectorizer:\n",
    "    def __init__(self, token_pattern, min_df=1, max_df=1.0, stop_words=None):\n",
    "        self.token_pattern = token_pattern\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.vocabulary = None\n",
    "        self.idf = None\n",
    "\n",
    "    def _preprocess(self, texts):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            text = text.lower()\n",
    "            words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "            processed_texts.append(words)\n",
    "        return processed_texts\n",
    "\n",
    "    def _build_vocabulary(self, processed_texts):\n",
    "        doc_count = len(processed_texts)\n",
    "        word_doc_freq = defaultdict(int)\n",
    "        for words in processed_texts:\n",
    "            unique_words = set(words)\n",
    "            for word in unique_words:\n",
    "                word_doc_freq[word] += 1\n",
    "        \n",
    "        filtered_words = [\n",
    "            word for word, freq in word_doc_freq.items() \n",
    "            if freq >= 1 and freq <= doc_count\n",
    "        ]\n",
    "        self.vocabulary = {\n",
    "            word: idx for idx, word in enumerate(sorted(filtered_words))\n",
    "        }\n",
    "        self.idf = np.zeros(len(self.vocabulary))\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            doc_freq = word_doc_freq[word]\n",
    "            self.idf[idx] = np.log((doc_count + 1) / (doc_freq + 1)) + 1\n",
    "\n",
    "    def _compute_tf(self, words):\n",
    "        total_words = len(words)\n",
    "        if total_words == 0:\n",
    "            return {}\n",
    "        word_freq = defaultdict(int)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        tf = {word: freq / total_words for word, freq in word_freq.items()}\n",
    "        return tf\n",
    "    def fit(self, texts):\n",
    "        processed_texts = self._preprocess(texts)\n",
    "        self._build_vocabulary(processed_texts)\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        processed_texts = self._preprocess(texts)\n",
    "        n_docs = len(processed_texts)\n",
    "        n_features = len(self.vocabulary)\n",
    "        tfidf_matrix = np.zeros((n_docs, n_features))\n",
    "        for doc_idx, words in enumerate(processed_texts):\n",
    "            tf = self._compute_tf(words)\n",
    "            for word, tf_value in tf.items():\n",
    "                if word in self.vocabulary:\n",
    "                    word_idx = self.vocabulary[word]\n",
    "                    tfidf_matrix[doc_idx, word_idx] = tf_value * self.idf[word_idx]\n",
    "        return tfidf_matrix\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        return self.fit(texts).transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "3aa4f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = MyTfidfVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "X_train = vectorizer.fit_transform(X_doc_train)\n",
    "X_test = vectorizer.transform(X_doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "1ee967c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMaxAbsScaler:\n",
    "    def fit(self, X):\n",
    "        self.max_abs = np.max(np.abs(X), axis=0)\n",
    "        self.max_abs[self.max_abs == 0] = 1.0\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_scaled = X / self.max_abs \n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVM:\n",
    "    def __init__(self, C=1.0, n_iters=1000, lr=0.01, gamma=0.8, tol=1e-4,):\n",
    "        self.C = C\n",
    "        self.n_iters = n_iters\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.scaler = MyMaxAbsScaler()\n",
    "        self.gamma = gamma\n",
    "    def _lr_scheduler(self, epoch):\n",
    "        return self.lr*(self.gamma**epoch)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        n_features = X.shape[1]\n",
    "        labels = np.where(y == 0, -1, 1)\n",
    "        self.w =np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        velocity_w = np.zeros_like(self.w)\n",
    "        velocity_b = 0.0\n",
    "        for epoch in range(self.n_iters):\n",
    "            margins = labels * (np.dot(X, self.w) + self.b)\n",
    "            support_vectors = margins < 1\n",
    "            if np.any(support_vectors):\n",
    "                grad_w = self.C * self.w - np.mean(\n",
    "                    labels[support_vectors][:, np.newaxis] * X[support_vectors], axis=0\n",
    "                )\n",
    "                grad_b = -np.mean(labels[support_vectors])\n",
    "            else:\n",
    "                grad_w = self.C * self.w\n",
    "                grad_b = 0\n",
    "            w_prev = self.w.copy()\n",
    "            b_prev = self.b\n",
    "            lr = self._lr_scheduler(epoch)\n",
    "            momentum = 0.9\n",
    "            velocity_w = momentum * velocity_w + grad_w\n",
    "            velocity_b = momentum * velocity_b + grad_b\n",
    "            grad_w += momentum * velocity_w\n",
    "            grad_b += momentum * velocity_b\n",
    "            self.w -= lr * grad_w\n",
    "            self.b -= lr * grad_b\n",
    "            \n",
    "            if np.linalg.norm(self.w - w_prev) < self.tol and abs(self.b - b_prev) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        approx = np.dot(X, self.w) + self.b\n",
    "        return np.where(approx >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "461e2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        80\n",
      "           1       1.00      0.99      0.99        80\n",
      "\n",
      "    accuracy                           0.99       160\n",
      "   macro avg       0.99      0.99      0.99       160\n",
      "weighted avg       0.99      0.99      0.99       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_svm = MySVM()\n",
    "my_svm.fit(X_train, y_train)\n",
    "pred_my_svm = my_svm.predict(X_test)\n",
    "print(classification_report(y_test, pred_my_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "974db7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98        80\n",
      "           1       0.98      0.99      0.98        80\n",
      "\n",
      "    accuracy                           0.98       160\n",
      "   macro avg       0.98      0.98      0.98       160\n",
      "weighted avg       0.98      0.98      0.98       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sk_svm = LinearSVC()\n",
    "sk_svm.fit(X_train, y_train)\n",
    "pred_sk_svm = sk_svm.predict(X_test)\n",
    "print(classification_report(y_test, pred_sk_svm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
